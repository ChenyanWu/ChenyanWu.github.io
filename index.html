<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Chenyan Wu</title>

    <meta name="author" content="Chenyan Wu">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Chenyan Wu
                </p>
                <!-- <p>I am a senior staff research scientist at <a href="https://ai.google/research">Google Research</a> in <a href="https://en.wikipedia.org/wiki/One_Market_Plaza">San Francisco</a>, where I work on computer vision and machine learning.
                </p>
                <p>
                  At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://www.matthewtancik.com/nerf">NeRF</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:czw390@psu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=wDtB5FIAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/jon_barron">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jonbarron/">Github</a>
                </p> -->
              </td>
              <td style="padding:2.5%;width:30%;max-width:30%">
                <a href="images/chenyanwu.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/chenyanwu.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
            <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                    <span class="papertitle">Cleaning the USNO-B Catalog Through Automatic Detection of Optical Artifacts</span>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                  <br>
                  <em>The Astronomical Journal</em>, 135, 2008
                  <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>
                  <p>In use at <a href="http://www.astrometry.net">Astrometry.net</a></p>
                </td>
            </tr> -->

            <tr bgcolor="#ffffd0">
                <td style="padding:15px;width:35%;vertical-align:middle">
                    <img src="images/p-ieee.png" alt="clean-usnob" style="width:100%; height:auto;">
                </td>
                <td style="padding:15px;width:65%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/10132377">
                    <span class="papertitle">Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion</span>
                </a>
                <br>
                James Z. Wang, Sicheng Zhao, <strong>Chenyan Wu*</strong>, Reginald B. Adams, Jr., Michelle G. Newman, Tal Shafir, Rachelle Tsachor
                <br>
                <em>Proceedings of the IEEE</em>, 2023 &nbsp&nbsp&nbsp(<strong>*</strong>sole student author)
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/10132377">paper</a>
                /
                <a href="https://arxiv.org/abs/2307.13463">arXiv</a>
                <p>
                This 51-page article provides a comprehensive overview of the field of emotion analysis in visual media and discusses the latest research, challenges, and potential impact of artificial emotional intelligence on society.
                </p>
                </td>
            </tr>

            <tr onmouseout="darkflash_stop()" onmouseover="darkflash_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='darkflash_image'><img src='images/darkflash_after.png'></div>
                  <img src='images/darkflash_before.png'>
                </div>
                <script type="text/javascript">
                  function darkflash_start() {
                    document.getElementById('darkflash_image').style.opacity = "1";
                  }

                  function darkflash_stop() {
                    document.getElementById('darkflash_image').style.opacity = "0";
                  }
                  darkflash_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1901.01370">
                  <span class="papertitle">Stereoscopic Dark Flash for Low-light Photography</span>
                </a>
                <br>
                <a href="https://www.andrew.cmu.edu/user/jianwan2/">Jian Wang</a>,
                <a href="https://tianfan.info/">Tianfan Xue</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>
                <br>
                <em>ICCP</em>, 2019
                <br>
                <p></p>
                <p>
                  By making one camera in a stereo pair hyperspectral we can multiplex dark flash pairs in space instead of time.
                </p>
              </td>
            </tr>

            <tr onmouseout="motionstereo_stop()" onmouseover="motionstereo_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='motionstereo_image'><img src='images/motionstereo_after.png'></div>
                  <img src='images/motionstereo_before.png'>
                </div>
                <script type="text/javascript">
                  function motionstereo_start() {
                    document.getElementById('motionstereo_image').style.opacity = "1";
                  }

                  function motionstereo_stop() {
                    document.getElementById('motionstereo_image').style.opacity = "0";
                  }
                  motionstereo_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1AABFJ3NgD5DAo5JEpEjWZrcQNzjZnvW9/view?usp=sharing">
                  <span class="papertitle">Depth from Motion for Smartphone AR</span>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/valentinjulien/">Julien Valentin</a>,
                <a href="https://www.linkedin.com/in/adarshkowdle/">Adarsh Kowdle</a>,
                <strong>Jonathan T. Barron</strong>, <a href="http://nealwadhwa.com">Neal Wadhwa</a>, and others
                <br>
                <em>SIGGRAPH Asia</em>, 2018
                <br>
                <a href="https://github.com/jonbarron/planar_filter">planar filter toy code</a> / 
                <a href="data/Valentin2018.bib">bibtex</a>
                <p></p>
                <p>Depth cues from camera motion allow for real-time occlusion effects in augmented reality applications.</p>
              </td>
            </tr>

            <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='portrait_image'><img src='images/portrait_after.jpg'></div>
                  <img src='images/portrait_before.jpg'>
                </div>
                <script type="text/javascript">
                  function portrait_start() {
                    document.getElementById('portrait_image').style.opacity = "1";
                  }

                  function portrait_stop() {
                    document.getElementById('portrait_image').style.opacity = "0";
                  }
                  portrait_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/13i6DlS9UhGVKmwslLUFnKBwdxFRVQeQj/view?usp=sharing">
                  <span class="papertitle">Synthetic Depth-of-Field with a Single-Camera Mobile Phone</span>
                </a>
                <br>
                <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                <a href="http://graphics.stanford.edu/~dejacobs/">David E. Jacobs</a>, Bryan E. Feldman, Nori Kanazawa, Robert Carroll,
                <a href="http://www.cs.cmu.edu/~ymovshov/">Yair Movshovitz-Attias</a>,
                <strong>Jonathan T. Barron</strong>, Yael Pritch,
                <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>
                <br>
                <em>SIGGRAPH</em>, 2018
                <br>
                <a href="https://arxiv.org/abs/1806.04171">arxiv</a> /
                <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">blog post</a> /
                <a href="data/Wadhwa2018.bib">bibtex</a>
                <p></p>
                <p>Dual pixel cameras and semantic segmentation algorithms can be used for shallow depth of field effects.</p>
                <p>This system is the basis for "Portrait Mode" on the Google Pixel 2 smartphones</p>
              </td>
            </tr>
            
            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
