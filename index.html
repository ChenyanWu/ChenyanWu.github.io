<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Chenyan Wu</title>

    <meta name="author" content="Chenyan Wu">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Chenyan Wu
                </p>
                <p>I am currently a final year Ph.D Candidate in <a href="https://ist.psu.edu/">College of Information Sciences & Technology</a> from <a href="https://www.psu.edu/">The Pennsylvania State University</a>, advised by <em>Prof. <a href="http://wang.ist.psu.edu/docs/home.shtml">James Z. Wang</a></em>. Prior to joining Penn Sate, I received my B.E in Electronic Information Engineering from <a href="http://en.scgy.ustc.edu.cn/">School of the Gifted Young</a>, <a href="http://en.ustc.edu.cn/">University of Science and Technology of China</a>. I have also spent wonderful time as an intern in Amazon Astro, Amazon Alexa, Microsoft Research Asia, and SenseTime Research.
                <!-- </p>
                <p>I am a senior staff research scientist at <a href="https://ai.google/research">Google Research</a> in <a href="https://en.wikipedia.org/wiki/One_Market_Plaza">San Francisco</a>, where I work on computer vision and machine learning.
                </p>
                <p>
                  At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://www.matthewtancik.com/nerf">NeRF</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
                </p> -->
                <p style="text-align:center">
                  <a href="mailto:czw390@psu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Chenyan_cv.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=wDtB5FIAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/chenyan-wu-2a591615b/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ChenyanWu">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:30%;max-width:30%">
                <a href="images/chenyanwu.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/chenyanwu.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision and affective computing. Much of my research is about modeling and understand human behaviors from images or videos, such as human bodily emotion understand, 2D/3D human pose estimation. Representative papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr bgcolor="#ffffd0">
                <td style="padding:15px;width:35%;vertical-align:middle;height:160px;overflow:hidden;">
                    <img src="images/p-ieee.png" alt="" style="width:100%">
                </td>
                <td style="padding:15px;width:65%;vertical-align:middle">
                <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/abstract/document/10132377">
                    <span class="papertitle">Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion</span>
                </a>
                <br>
                James Z. Wang, Sicheng Zhao, <strong>Chenyan Wu*</strong>, Reginald B. Adams, Michelle G. Newman, Tal Shafir, Rachelle Tsachor
                <br>
                <em>Proceedings of the IEEE</em>, 2023 &nbsp&nbsp&nbsp(<strong>*</strong>sole student author)
                <br>
                <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/abstract/document/10132377">paper</a>
                /
                <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2307.13463">arXiv</a>
                <p>
                This 51-page article provides a comprehensive overview of the field of emotion analysis in visual media and discusses the latest research, challenges, and potential impact of artificial emotional intelligence on society.
                </p>
                </td>
            </tr>
            
            <tr bgcolor="#ffffd0">
              <td style="padding:15px;width:35%;vertical-align:middle;">
                  <img src="images/patterns.png" alt="" style="width:100%">
              </td>
              <td style="padding:15px;width:65%;vertical-align:middle">
              <a target="_blank" rel="noopener noreferrer" href="https://www.cell.com/patterns/fulltext/S2666-3899(23)00185-X">
                  <span class="papertitle">Bodily Expressed Emotion Understanding Through Integrating Laban Movement Analysis</span>
              </a>
              <br>
              <strong>Chenyan Wu</strong>, Dolzodmaa Davaasuren, Tal Shafir, Rachelle Tsachor, James Z. Wang
              <br>
              <em>Patterns, Cell Press</em>, 2023
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://www.cell.com/patterns/fulltext/S2666-3899(23)00185-X">paper</a>
              /
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2304.02187">arXiv</a>
              /
              <a target="_blank" rel="noopener noreferrer" href="https://data.mendeley.com/datasets/gbhpdkf8pg/1">data</a>
              /
              <a target="_blank" rel="noopener noreferrer" href="https://github.com/ChenyanWu/BoME">code</a>
              <p>
              </p>
              </td>
            </tr>

            <tr bgcolor="#ffffd0">
              <td style="padding:15px;width:35%;vertical-align:middle;height:160px;overflow:hidden;">
                  <img src="images/meta.png" alt="" style="width:100%">
              </td>
              <td style="padding:15px;width:65%;vertical-align:middle">
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2303.01630">
                  <span class="papertitle">Learning to Adapt to Online Streams with Distribution Shifts</span>
              </a>
              <br>
              <strong>Chenyan Wu</strong>, Yimu Pan, Yandong Li, James Z. Wang
              <br>
              <em>arXiv</em>, 2023 &nbsp&nbsp&nbsp(under peer review)
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2303.01630">arXiv</a>
              <p>
              </p>
              </td>
            </tr>

            <tr bgcolor="#ffffd0">
              <td style="padding:15px;width:35%;vertical-align:middle;height:130px;overflow:hidden;">
                  <img src="images/mug.jpg" alt="" style="width:100%">
              </td>
              <td style="padding:15px;width:65%;vertical-align:middle">
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2205.12583">
                  <span class="papertitle">MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose</span>
              </a>
              <br>
              <strong>Chenyan Wu</strong>, Yandong Li, Xianfeng Tang, James Z. Wang
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2205.12583">arXiv</a>
              <p>
              </p>
              </td>
            </tr>

            <tr>
              <td style="padding:15px;width:35%;vertical-align:middle;height:160px;overflow:hidden;">
                  <img src="images/VOT.png" alt="" style="width:100%">
              </td>
              <td style="padding:15px;width:65%;vertical-align:middle">
              <a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content/ICCV2021W/VOT/html/Kristan_The_Ninth_Visual_Object_Tracking_VOT2021_Challenge_Results_ICCVW_2021_paper.html">
                  <span class="papertitle">The Ninth Visual Object Tracking VOT2021 Challenge Results</span>
              </a>
              <br>
              Matej Kristan, Ji≈ô√≠ Matas, ..., <strong>Chenyan Wu</strong>, et al.
              <br>
              <em>IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</em>, 2021
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content/ICCV2021W/VOT/html/Kristan_The_Ninth_Visual_Object_Tracking_VOT2021_Challenge_Results_ICCVW_2021_paper.html">paper</a>
              <p>
              </p>
              </td>
            </tr>

            <tr bgcolor="#ffffd0">
              <td style="padding:15px;width:35%;vertical-align:middle;height:160px;overflow:hidden;">
                <img src="images/MEBOW1.png" alt="" style="width:100%">
                <!-- <img src="images/MEBOW2.png" alt="second-image" style="width:20%; display: inline-block;"> -->
              </td>
              <td style="padding:15px;width:65%;vertical-align:middle">
              <a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Wu_MEBOW_Monocular_Estimation_of_Body_Orientation_in_the_Wild_CVPR_2020_paper.html">
                  <span class="papertitle">MEBOW: Monocular Estimation of Body Orientation In the Wild</span>
              </a>
              <br>
              <strong>Chenyan Wu</strong>, Yukun Chen, Jiajia Luo, Che-Chun Su, Anuja Dawane, Bikramjot Hanzra, Zhuo Deng, Bilan
              Liu, James Z. Wang, Cheng-hao Kuo
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Wu_MEBOW_Monocular_Estimation_of_Body_Orientation_in_the_Wild_CVPR_2020_paper.html">paper</a>
              /
              <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.13688">arXiv</a>
              /
              <a target="_blank" rel="noopener noreferrer" href="https://chenyanwu.github.io/MEBOW">data</a>
              /
              <a target="_blank" rel="noopener noreferrer" href="https://github.com/ChenyanWu/MEBOW">code</a>
              <p>
              </p>
              </td>
            </tr>

            <tr>
              <td style="padding:15px;width:35%;vertical-align:middle;height:160px;overflow:hidden;">
                  <img src="images/cmig.png" alt="" style="width:100%">
              </td>
              <td style="padding:15px;width:65%;vertical-align:middle">
              <a target="_blank" rel="noopener noreferrer" href="https://www.sciencedirect.com/science/article/pii/S0895611120300471">
                  <span class="papertitle">AI-PLAX: AI-based Placental Assessment and Examination Using Photos</span>
              </a>
              <br>
              Yukun Chen, Zhuomin Zhang, <strong>Chenyan Wu</strong>, Dolzodmaa Davaasuren, Jeffery Goldstein, Alison Gernand, James Z. Wang
              <br>
              <em>Computerized Medical Imaging and Graphics (CMIG)</em>, 2020
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://www.sciencedirect.com/science/article/pii/S0895611120300471">paper</a>
              <p>
              </p>
              </td>
            </tr>

            <tr>
              <td style="padding:15px;width:35%;vertical-align:middle;height:160px;overflow:hidden;">
                  <img src="images/prl_2.png" alt="" style="width:100%">
              </td>
              <td style="padding:15px;width:65%;vertical-align:middle">
              <a target="_blank" rel="noopener noreferrer" href="https://www.sciencedirect.com/science/article/pii/S0167865520303780">
                  <span class="papertitle"> Multi-region Saliency-aware Learning for Cross-domain Placenta Image Segmentation</span>
              </a>
              <br>
              Zhuomin Zhang, Dolzodmaa Davaasuren, <strong>Chenyan Wu</strong>, Jeffery Goldstein, Alison Gernand, James Z. Wang
              <br>
              <em>Pattern Recognition Letters (PRL)</em>, 2020
              <br>
              <a target="_blank" rel="noopener noreferrer" href="https://www.sciencedirect.com/science/article/pii/S0167865520303780">paper</a>
              <p>
              </p>
              </td>
            </tr>

            <tr>
              <td style="padding:15px;width:35%;vertical-align:middle;height:160px;overflow:hidden;">
                  <img src="images/miccai.png" alt="" style="width:100%">
              </td>
              <td style="padding:15px;width:65%;vertical-align:middle">
              <a target="_blank" rel="noopener noreferrer" href="https://link.springer.com/chapter/10.1007/978-3-030-32239-7_54">
                  <span class="papertitle">PlacentaNet: Automatic Morphological Characterization of Placenta Photos with Deep Learning</span>
              </a>
              <br>
              Yukun Chen, <strong>Chenyan Wu</strong>, Zhuomin Zhang, Jeffery Goldstein, Alison Gernand, James Z. Wang
              <br>
              <em>Medical Image Computing and Computer Assisted Intervention (MICCAI)</em>, 2019
              <br>
              <a target="_blank" rel="noopener noreferrer" href="http://infolab.stanford.edu/~wangz/project/imsearch/MEDICAL/MICCAI19/chen2.pdf">paper</a>
              <p>
              </p>
              </td>
            </tr>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Template adapted from <a href="https://github.com/jonbarron/jonbarron_website">this awesome website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
